{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized value: 1.0000000040440504\n",
      "Function value at optimized value: -1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    return x**3 - 3*x + 1\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([0])\n",
    "\n",
    "# Perform the optimization\n",
    "result = minimize(objective_function, x0)\n",
    "\n",
    "# Output the result\n",
    "if result.success:\n",
    "    optimized_value = result.x[0]\n",
    "    print(f\"Optimized value: {optimized_value}\")\n",
    "    print(f\"Function value at optimized value: {objective_function(optimized_value)}\")\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGS:   message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 1.8932783589357527e-11\n",
      "        x: [ 1.000e+00  1.000e+00]\n",
      "      nit: 30\n",
      "      jac: [ 5.274e-06 -2.506e-06]\n",
      " hess_inv: [[ 5.168e-01  1.032e+00]\n",
      "            [ 1.032e+00  2.066e+00]]\n",
      "     nfev: 105\n",
      "     njev: 35\n",
      "Nelder-Mead:        message: Optimization terminated successfully.\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: 3.4239552925089766e-10\n",
      "             x: [ 1.000e+00  1.000e+00]\n",
      "           nit: 62\n",
      "          nfev: 119\n",
      " final_simplex: (array([[ 1.000e+00,  1.000e+00],\n",
      "                       [ 1.000e+00,  1.000e+00],\n",
      "                       [ 1.000e+00,  1.000e+00]]), array([ 3.424e-10,  6.041e-10,  8.954e-10]))\n"
     ]
    }
   ],
   "source": [
    "# Define the Rosenbrock function\n",
    "def rosenbrock(x):\n",
    "    a = 1\n",
    "    b = 100\n",
    "    return (a - x[0])**2 + b*(x[1] - x[0]**2)**2\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([2, 2])\n",
    "\n",
    "# Minimize using BFGS\n",
    "res_bfgs = minimize(rosenbrock, x0, method='BFGS')\n",
    "print(\"BFGS:\", res_bfgs)\n",
    "\n",
    "# Minimize using Nelder-Mead\n",
    "res_nelder = minimize(rosenbrock, x0, method='Nelder-Mead')\n",
    "print(\"Nelder-Mead:\", res_nelder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BFGS Output:**\n",
    "<br> **message:** \"Optimization terminated successfully.\" This indicates that the optimization process finished without any errors.\n",
    "<br>**success:** True. The algorithm reached a solution that it considers optimal based on the algorithm's criteria.\n",
    "<br>status:** 0. This is a code representing the reason the optimizer terminated. In this case, 0 usually means successful completion.\n",
    "<br>**fun:** The value of the Rosenbrock function at the optimal point found by the algorithm. It is extremely close to 0, which is the minimum value of the function.\n",
    "<br>**x:** The optimal solution found by the algorithm. It is an array with values [1., 1.], which is the global minimum of the Rosenbrock function at (1,1).\n",
    "<br>**nit:** Number of iterations. The algorithm took 30 iterations to converge to the optimal solution.\n",
    "<br>**jac:** The value of the Jacobian (first derivative) of the Rosenbrock function at the optimal point, which is close to zero as expected at a minimum.\n",
    "<br>**hess_inv:** The inverse of the Hessian matrix at the optimal point. The Hessian matrix provides second-order information (curvature), and its inverse is used in BFGS to update the estimate of the optimal point.\n",
    "<br>**nfev:** Number of function evaluations. The function was evaluated 105 times.\n",
    "<br>**njev:** Number of Jacobian evaluations. The Jacobian was evaluated 35 times.\n",
    "<br>\n",
    "<br>**Nelder-Mead Output:**\n",
    "<br>**message:** \"Optimization terminated successfully.\" Like with BFGS, this indicates that the optimization process finished without any errors.\n",
    "<br>**success:** True. The algorithm found a solution that meets its convergence criteria.\n",
    "<br>**status:** 0. As with BFGS, this status code indicates success.\n",
    "<br>**fun:** The final value of the Rosenbrock function, again very close to 0, indicating that the Nelder-Mead method also found the valley.\n",
    "<br>**x:** The optimal solution found [1., 1.], which is consistent with the BFGS result and confirms the function's global minimum.\n",
    "<br>**nit:** Number of iterations. The algorithm took 62 iterations, which is more than BFGS. This is typical since Nelder-Mead doesn't use gradient information and can require more iterations.\n",
    "<br>**nfev:** Number of function evaluations. The function was evaluated 119 times.\n",
    "<br>**final_simplex:** The last simplex of the algorithm. A simplex in Nelder-Mead is a geometric figure (in this case, a triangle in 2 dimensions) that the method uses to probe the function's landscape. The vertices of the final simplex are very close to the minimum, and the associated function values are given in the array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
