{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import numpy.linalg as la \n",
    "import gurobipy as gp\n",
    "\n",
    "from scipy.optimize import minimize, newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To convert to .pdf from Jupyter notebook**\n",
    "\n",
    "jupyter nbconvert --to webpdf --allow-chromium-download Untitled.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvalue determination from matrix**\n",
    "- All eigenvalues of $A$ are positive $f(x)$ is strictly convex\n",
    "- All eigenvalues of $A$ are non-negative $f(x)$ is convex, which may not be strict\n",
    "- All eigenvalues of $A$ are negative $f(x)$ is strictly concave\n",
    "- All eigenvalues of $A$ are non-positive $f(x)$ is concave, which may not be strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45861873 6.54138127]\n"
     ]
    }
   ],
   "source": [
    "m = np.matrix([[3, 3],[3,4]])\n",
    "print(np.linalg.eigvals(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quadratic Constrained**\n",
    "\n",
    "For quadratically constrained optimization problems, either for a quadratic objective functions or quadratic constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "m = gp.Model(\"qcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (win64)\n",
      "\n",
      "CPU model: 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 4 rows, 3 columns and 6 nonzeros\n",
      "Model fingerprint: 0x8ada3258\n",
      "Model has 3 quadratic constraints\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  QMatrix range    [1e-03, 5e-03]\n",
      "  QLMatrix range   [4e-02, 3e-01]\n",
      "  Objective range  [1e-02, 7e-02]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [3e+01, 1e+02]\n",
      "  QRHS range       [5e-01, 1e+00]\n",
      "Presolve removed 3 rows and 0 columns\n",
      "Presolve time: 0.02s\n",
      "Presolved: 6 rows, 7 columns, 15 nonzeros\n",
      "Presolved model has 1 second-order cone constraint\n",
      "Ordering time: 0.00s\n",
      "\n",
      "Barrier statistics:\n",
      " AA' NZ     : 1.500e+01\n",
      " Factor NZ  : 2.100e+01\n",
      " Factor Ops : 9.100e+01 (less than 1 second per iteration)\n",
      " Threads    : 1\n",
      "\n",
      "                  Objective                Residual\n",
      "Iter       Primal          Dual         Primal    Dual     Compl     Time\n",
      "   0   9.22718340e+00  3.84850306e+00  7.10e+00 1.73e-01  9.96e-01     0s\n",
      "   1   5.31919506e+00  4.58278944e+00  1.05e+00 1.91e-07  1.11e-01     0s\n",
      "   2   5.55487282e+00  5.05956538e+00  7.37e-07 2.55e-08  4.95e-02     0s\n",
      "   3   5.36022889e+00  5.34637827e+00  8.01e-09 2.80e-14  1.39e-03     0s\n",
      "   4   5.35683574e+00  5.35681966e+00  2.26e-13 6.47e-16  1.61e-06     0s\n",
      "   5   5.35683233e+00  5.35683231e+00  1.78e-15 3.15e-17  1.77e-09     0s\n",
      "\n",
      "Barrier solved model in 5 iterations and 0.02 seconds (0.00 work units)\n",
      "Optimal objective 5.35683233e+00\n",
      "\n",
      "r 45.23\n",
      "o 4.77\n",
      "c 50.00\n",
      "Obj: 5.36\n"
     ]
    }
   ],
   "source": [
    "# Create variables\n",
    "r = m.addVar(vtype=gp.GRB.CONTINUOUS, name=\"r\")\n",
    "o = m.addVar(vtype=gp.GRB.CONTINUOUS, name=\"o\")\n",
    "c = m.addVar(vtype=gp.GRB.CONTINUOUS, name=\"c\")\n",
    "\n",
    "# Set objective: 0.04*r + 0.01*o + 0.07*c\n",
    "obj = 0.04*r + 0.01*o + 0.07*c\n",
    "m.setObjective(obj, gp.GRB.MINIMIZE)\n",
    "\n",
    "# Add all linear constraints: r + o + c == 100, c >= 50, r >= 25, r <= 60\n",
    "m.addConstr(r + o + c == 100, \"c0\")\n",
    "m.addConstr(c >= 50, \"c1\")\n",
    "m.addConstr(r >= 25, \"c2\")\n",
    "m.addConstr(r <= 60, \"c3\")\n",
    "\n",
    "# Add tensile strength constraint: 0.001 o^2 + 0.10o <= 0.5\n",
    "m.addConstr(0.001*o*o + 0.10*o <= 0.5, \"qc0\")\n",
    "\n",
    "# Add elasticity constraint: 0.002o^2 - 0.35r + 0.04o <= 1\n",
    "m.addConstr(0.002*o*o - 0.35*r + 0.04*o <= 1, \"qc1\")\n",
    "\n",
    "# Add hardness constraint: 0.002r^2 + 0.005o^2 + 0.001c^2 + 0.001r*o + 0.10r + 0.06o - 0.3c <= 1\n",
    "m.addConstr(0.002*r*r + 0.005*o*o + 0.001*c*c + 0.001*r*o + 0.10*r + 0.06*o - 0.3*c <= 1, \"qc2\")\n",
    "\n",
    "# # To be used for Nonconvex problems in gurobi\n",
    "# m.setParam('Nonconvex',2)\n",
    "m.optimize()\n",
    "\n",
    "for v in m.getVars():\n",
    "  print('%s %.2f' % (v.varName, v.x)) # rounded to 2 decimals\n",
    "\n",
    "print('Obj: %.2f' % obj.getValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-Quadratic Unconstrained**\n",
    "\n",
    "Leveraging scipy.optimize.minimize function that accepts a function f0, an initial point x0 and corresponding methods:\n",
    "\n",
    "`opt = scipy.optimize.minimize(f0, x0, method='CG')`\n",
    "\n",
    "- `minimize(method='CG')`: Conjugate Gradient algorithm, good for large-scale problems.\n",
    "- `minimize(method='BFGS')`: Broyden–Fletcher–Goldfarb–Shannon algorithm, good for medium-sized problems.\n",
    "- `minimize(method='Newton-CG')`: Newton-Conjugate Gradient algorithm, good for large problems where the Hessian is sparse.\n",
    "- `minimize(method='Nelder-Mead')`: Simplex algorithm, good for non-differentiable problems.\n",
    "\n",
    "**Output Returns** \n",
    "\n",
    "- **BFGS Output:**\n",
    "    - **message:** \"Optimization terminated successfully.\" This indicates that the optimization process finished without any errors.\n",
    "    - **success:** True. The algorithm reached a solution that it considers optimal based on the algorithm's criteria.\n",
    "    - **status:** 0. This is a code representing the reason the optimizer terminated. In this case, 0 usually means successful completion.\n",
    "    - **fun:** The value of function at the optimal point found by the algorithm.\n",
    "    - **x:** The optimal solution found by the algorithm.\n",
    "    - **nit:** Number of iterations.\n",
    "    - **jac:** The value of the Jacobian (first derivative) of the function at the optimal point.\n",
    "    - **hess_inv:** The inverse of the Hessian matrix at the optimal point. The Hessian matrix provides second-order information (curvature).\n",
    "    - **nfev:** Number of function evaluations.\n",
    "    - **njev:** Number of Jacobian evaluations.\n",
    "\n",
    "\n",
    "- **Nelder-Mead Output:**\n",
    "    - **message:** \"Optimization terminated successfully.\" This indicates that the optimization process finished without any errors.\n",
    "    - **success:** True. The algorithm found a solution that meets its convergence criteria.\n",
    "    - **status:** 0. This status code indicates success.\n",
    "    - **fun:** The final value of the function, again very close to 0, indicating that the Nelder-Mead method also found the valley.\n",
    "    - **x:** The optimal solution found.\n",
    "    - **nit:** Number of iterations.\n",
    "    - **nfev:** Number of function evaluations.\n",
    "    - **final_simplex:** The last simplex of the algorithm. The vertices of the final simplex are very close to the minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGS:       fun: 1.8932820837847567e-11\n",
      " hess_inv: array([[0.51676073, 1.03186646],\n",
      "       [1.03186646, 2.06557555]])\n",
      "      jac: array([ 5.27409301e-06, -2.50588723e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 105\n",
      "      nit: 30\n",
      "     njev: 35\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.99999565, 0.99999129])\n",
      "Nelder-Mead:       fun: 1.8932820837847567e-11\n",
      " hess_inv: array([[0.51676073, 1.03186646],\n",
      "       [1.03186646, 2.06557555]])\n",
      "      jac: array([ 5.27409301e-06, -2.50588723e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 105\n",
      "      nit: 30\n",
      "     njev: 35\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.99999565, 0.99999129])\n"
     ]
    }
   ],
   "source": [
    "# Define the Rosenbrock function\n",
    "def rosenbrock(x):\n",
    "    a = 1\n",
    "    b = 100\n",
    "    return (a - x[0])**2 + b*(x[1] - x[0]**2)**2\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([2, 2])\n",
    "\n",
    "# Minimize using BFGS\n",
    "res_bfgs = minimize(rosenbrock, x0, method='BFGS')\n",
    "print(\"BFGS:\", res_bfgs)\n",
    "\n",
    "# Minimize using Nelder-Mead\n",
    "res_nelder = minimize(rosenbrock, x0, method='BFGS')\n",
    "print(\"Nelder-Mead:\", res_nelder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Constraints with the Scipy package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider a minimization problem with several constraints\n",
    "fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
    "\n",
    "# Constraints for COBYLA, SLSQP are defined as a list of dictionaries. Each dictionary with fields:\n",
    "# Note all conditions are >= 0 for a given condition\n",
    "# Constraint type: ‘eq’ for equality, ‘ineq’ for inequality.\n",
    "cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
    "        {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
    "        {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
    "\n",
    "# And variables must be positive, hence the following bounds:\n",
    "bnds = ((0, None), (0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
    "               constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.8000000011920985\n",
      "     jac: array([ 0.80000002, -1.59999999])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 10\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.4, 1.7])\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steepest  Descent (unconstrained NLP)**\n",
    "\n",
    "Consider a general unconstrained nonlinear optimization problem with convex, continuous, and twice differentiable objective function and convex feasible region. Generally, Steepest Descent starts from an initial point $x^0$ that we want to move in an improving direction $\\Delta x$\n",
    "\n",
    "The step direction $-H^{-1} \\nabla f$ (rather than just the negative of thegradient ) is called the deflected gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Base Function**: $f(x)=x_1^2 + x_1x_2 + 3x_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Define the objective function\n",
    "    return x[0] ** 2 + x[0] * x[1] + 3 * x[1] ** 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient**: $\\nabla f(x)=[2x_1+x_2, x_1+6x_2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    # Define the gradient of the objective function\n",
    "    return np.array([2 * x[0] + x[1], x[0] + 6 * x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H=\\begin{bmatrix} \\frac{\\partial f^2}{\\partial x_1x_1} & \\frac{\\partial f^2}{\\partial x_1x_2} \\\\ \\frac{\\partial f^2}{\\partial x_2x_1} & \\frac{\\partial f^2}{\\partial x_2x_2} \\end{bmatrix}=\\begin{bmatrix} 2 & 1 \\\\ 1 & 6 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H():\n",
    "    # Define the Hessian matrix of the objective function\n",
    "    return np.array([[2, 1], [1, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.81182167e-08, 1.81182167e-08])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton(f, np.array([3, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.76393202, 6.23606798])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigvals(H())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(gradient, hessian, x0:list=[np.array([1, 1])], opt_fun:str='min', \n",
    "                  maxiter:int=1000, t_opt:float = 0.1, tol:float = 1e-6, dxmin:float = 1e-6, \n",
    "                  dx:float=float('Inf'), gnorm:float= float('Inf'), verbose=False):\n",
    "    \"\"\"\n",
    "    Newton Method Algorithm\n",
    "    \n",
    "    Input:\n",
    "        gradient : Define the gradient of the objective function\n",
    "        hessian  : Define the hessian matrix for the decomposition\n",
    "        x0       : Store an initial point \"x\" into it\n",
    "        maxiter  : Maximum number of iterations\n",
    "        t_opt    : Set fixed step size\n",
    "        tol      : Termination tolerance for minimimum/maximum to be achieved\n",
    "        dxmin    : Minimum allowed perturbation (i.e. change allowed in the x-variable step)\n",
    "        dx       : Set dx as a very large number\n",
    "    \n",
    "    Output:\n",
    "        x : Current optimal solution     \n",
    "        g : Gradient of the function at new point\n",
    "        n : Numbers of iterations\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0 \n",
    "    \n",
    "    # Main while loop: stay in this loop, while none of the following three conditions are satisfied:\n",
    "    # 1) The gradient norm is sufficiently larger than zero (if zero no further gain expected)\n",
    "    # 2) count is less than the max iteration count; (to prevent forever loop)\n",
    "    # 3) The change from x to the next point, is sufficiently large (when it gets small then we've reached termination)\n",
    "    \n",
    "    while (gnorm >= tol and (count <= maxiter and dx >= dxmin)):\n",
    "        x = x0[-1]  # Always set x equal to the most recently placed point\n",
    "        \n",
    "        # We provide our differntial x point as follows\n",
    "        if opt_fun == 'min':\n",
    "            g = -np.linalg.inv(hessian).dot(gradient(x))\n",
    "        else:\n",
    "            g = np.linalg.inv(hessian).dot(gradient(x))\n",
    "            \n",
    "        next_point = x + t_opt * g  # Update next point\n",
    "        x0.append(next_point)       # Append next_point to list of points\n",
    "\n",
    "        # Update termination conditions\n",
    "        gnorm = la.norm(-g)           # Compute norm of gradient (euclidean distance) to see direction improvement\n",
    "        dx = la.norm(next_point - x)  # Compute the norm of the difference between the last point x and the next point\n",
    "        count = count + 1             # Increment the iteration count\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{count}: The current point is: {x} => hessian-gradient: {g}\")\n",
    "        \n",
    "    return x, g, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.06236104e-06, 7.06236104e-06]),\n",
       " array([-7.06236104e-06, -7.06236104e-06]),\n",
       " 124)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_method(df, hessian=H(), x0=[np.array([3, 3])], t_opt=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_operator(x, cons_matrix, cons_rhs):\n",
    "    \"\"\"\n",
    "    Project the point onto the feasible set defined by Ax <= b\n",
    "    \n",
    "    Input: \n",
    "        x: base shift as computed from normal gradient descent\n",
    "        cons_matrix: the matrix form for linear constraints\n",
    "        cons_rhs: the right-hand side of the values\n",
    "        \n",
    "    Output:\n",
    "        projected_x: the projection given constraints (used to assign for steepest descent)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = cons_matrix\n",
    "    b = cons_rhs\n",
    "\n",
    "    projected_x = np.maximum(x, 0)  # Non-negativity constraint\n",
    "    projected_x = np.minimum(projected_x, np.linalg.lstsq(A.T, b, rcond=None)[0])\n",
    "\n",
    "    return projected_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(gradient, x0:list=[np.array([1, 1])], opt_fun:str='min', \n",
    "                     maxiter:int=1000, t_opt:float = 0.1, tol:float = 1e-6, dxmin:float = 1e-6, \n",
    "                     dx:float=float('Inf'), gnorm:float= float('Inf'), verbose=False):\n",
    "    \"\"\"\n",
    "    Gradient Descent Algorithm\n",
    "    \n",
    "    Input:\n",
    "        gradient : Define the gradient of the objective function\n",
    "        x0       : Store an initial point \"x\" into it\n",
    "        maxiter  : Maximum number of iterations\n",
    "        t_opt    : Set fixed step size\n",
    "        tol      : Termination tolerance for minimimum/maximum to be achieved\n",
    "        dxmin    : Minimum allowed perturbation (i.e. change allowed in the x-variable step)\n",
    "        dx       : Set dx as a very large number\n",
    "    \n",
    "    Output:\n",
    "        x : Current optimal solution     \n",
    "        g : Gradient of the function at new point\n",
    "        n : Numbers of iterations\n",
    "    \n",
    "    \"\"\"\n",
    "    assert opt_fun in ['min', 'max'], 'Optimization function is either min and max'\n",
    "    \n",
    "    count = 0 \n",
    "    \n",
    "    # Main while loop: stay in this loop, while none of the following three conditions are satisfied:\n",
    "    # 1) The gradient norm is sufficiently larger than zero (if zero no further gain expected)\n",
    "    # 2) count is less than the max iteration count; (to prevent forever loop)\n",
    "    # 3) The change from x to the next point, is sufficiently large (when it gets small then we've reached termination)\n",
    "    \n",
    "    while (gnorm >= tol and (count <= maxiter and dx >= dxmin)):\n",
    "        x = x0[-1]  # Always set x equal to the most recently placed point\n",
    "        \n",
    "        if opt_fun == 'min':\n",
    "            g = -gradient(x)  # For minimization problems, the negative of the gradient is the best improving direction\n",
    "        else:\n",
    "            g = gradient(x)\n",
    "        \n",
    "        next_point = x + t_opt * g  # Update next point\n",
    "        \n",
    "#         # To be used in terms of function constraints\n",
    "#         projection_operator(next_point, constraints, rhs)\n",
    "\n",
    "        x0.append(next_point)       # Append next_point to list of points\n",
    "\n",
    "        # Update termination conditions\n",
    "        gnorm = la.norm(-g)           # Compute norm of gradient (euclidean distance) to see direction improvement\n",
    "        dx = la.norm(next_point - x)  # Compute the norm of the difference between the last point x and the next point\n",
    "        count = count + 1             # Increment the iteration count\n",
    "        \n",
    "        if verbose:\n",
    "            print('----------')\n",
    "            print(f\"{count}: The current point is:{x}\\n => gradient: {g}\")\n",
    "           \n",
    "    return x, g, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_solution, latest_gradient, number_of_iterations = steepest_descent(df, x0=[np.array([3, 3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.89619491e-06, -1.15583483e-06])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Set Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of K: 0.9999999947525271\n",
      "Optimal value of L: 4.00000002098875\n",
      "Maximum value of z: 3.9999999999988587\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function (negated for minimization)\n",
    "def objective(x):\n",
    "    K, L = x\n",
    "    return -K * L  # Negate because we are minimizing\n",
    "\n",
    "# # Define the constraint\n",
    "# def constraint(x):\n",
    "#     K, L = x\n",
    "#     return 8 - (4 * K + L)\n",
    "\n",
    "# Define bounds for K and L\n",
    "bounds = [(0, None), (0, None)]  # Both K and L are greater than or equal to 0\n",
    "\n",
    "# Define initial guess\n",
    "x0 = [1, 1]  # Initial guess for K and L\n",
    "\n",
    "# Define the constraints in the format required by scipy\n",
    "# 4*K + L <= 8 ---> 4*K + L - 8 <= 0 ---> 0 <= -4*K - L + 8\n",
    "# 8 - (4 * K + L)\n",
    "constraints = [{'type': 'ineq', 'fun': lambda x: -4*x[0] - x[1] + 8}]\n",
    "\n",
    "# Run the optimization\n",
    "result = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "# Print the results\n",
    "if result.success:\n",
    "    optimized_K, optimized_L = result.x\n",
    "    print(f\"Optimal value of K: {optimized_K}\")\n",
    "    print(f\"Optimal value of L: {optimized_L}\")\n",
    "    print(f\"Maximum value of z: {-result.fun}\")  # Negate again to get the maximum\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter NonConvex to value 2\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (win64)\n",
      "\n",
      "CPU model: 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 13 rows, 18 columns and 31 nonzeros\n",
      "Model fingerprint: 0x55efe2aa\n",
      "Model has 8 quadratic constraints\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  QLMatrix range   [1e-02, 8e+02]\n",
      "  Objective range  [8e+01, 1e+02]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [5e+03, 1e+04]\n",
      "Presolve removed 6 rows and 0 columns\n",
      "\n",
      "Continuous model is non-convex -- solving as a MIP\n",
      "\n",
      "Presolve removed 6 rows and 2 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 27 rows, 17 columns, 82 nonzeros\n",
      "Presolved model has 8 bilinear constraint(s)\n",
      "Variable types: 17 continuous, 0 integer (0 binary)\n",
      "\n",
      "Root relaxation: objective 4.560000e+05, 11 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 456000.000    0    3          - 456000.000      -     -    0s\n",
      "H    0     0                    443237.01605 456000.000  2.88%     -    0s\n",
      "     0     0 452566.572    0    3 443237.016 452566.572  2.10%     -    0s\n",
      "     0     0 452566.572    0    3 443237.016 452566.572  2.10%     -    0s\n",
      "     0     0 452566.572    0    1 443237.016 452566.572  2.10%     -    0s\n",
      "     0     0 451490.467    0    4 443237.016 451490.467  1.86%     -    0s\n",
      "     0     0 451437.024    0    4 443237.016 451437.024  1.85%     -    0s\n",
      "     0     0 451341.451    0    4 443237.016 451341.451  1.83%     -    0s\n",
      "     0     0 451338.827    0    4 443237.016 451338.827  1.83%     -    0s\n",
      "     0     0 451318.995    0    4 443237.016 451318.995  1.82%     -    0s\n",
      "     0     0 451318.852    0    4 443237.016 451318.852  1.82%     -    0s\n",
      "     0     0 451317.789    0    4 443237.016 451317.789  1.82%     -    0s\n",
      "     0     0 451317.781    0    4 443237.016 451317.781  1.82%     -    0s\n",
      "     0     2 451317.781    0    4 443237.016 451317.781  1.82%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  RLT: 5\n",
      "\n",
      "Explored 5 nodes (48 simplex iterations) in 0.06 seconds (0.00 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 1: 443237 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 4.432370160528e+05, best bound 4.432573759205e+05, gap 0.0046%\n",
      "R: 5000.000000000278\n",
      "U: 5000.000000000432\n",
      "P: 11134.965600970616\n",
      "A1: 9047.619047617749\n",
      "A2: 952.380952382046\n",
      "T: 10999.999999999818\n",
      "L: 134.96560097171408\n",
      "SA: 0.037142857142853807\n",
      "OA: 91.57142857142925\n",
      "A: 9999.999999999794\n",
      "LP: 106.56665337640739\n",
      "TP: 5652.826981266505\n",
      "AP: 5375.571966327705\n",
      "TU: 2083.333333333349\n",
      "AU: 2916.6666666670835\n",
      "LR: 28.398947595306687\n",
      "TR: 3263.839685399965\n",
      "AR: 1707.7613670050066\n",
      "Obj: 443237.0160528358\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "\n",
    "import numpy as np\n",
    "from gurobipy import Model, GRB\n",
    "\n",
    "# Create a new model\n",
    "m = Model(\"optimization\")\n",
    "\n",
    "# Create variables\n",
    "R = m.addVar(vtype=GRB.CONTINUOUS, name=\"R\")\n",
    "U = m.addVar(vtype=GRB.CONTINUOUS, name=\"U\")\n",
    "P = m.addVar(vtype=GRB.CONTINUOUS, name=\"P\")\n",
    "Aa = m.addVar(vtype=GRB.CONTINUOUS, name=\"A1\")\n",
    "Ab = m.addVar(vtype=GRB.CONTINUOUS, name=\"A2\")\n",
    "T = m.addVar(vtype=GRB.CONTINUOUS, name=\"T\")\n",
    "L = m.addVar(vtype=GRB.CONTINUOUS, name=\"L\")\n",
    "SA = m.addVar(vtype=GRB.CONTINUOUS, name=\"SA\")\n",
    "OA = m.addVar(vtype=GRB.CONTINUOUS, name=\"OA\")\n",
    "A = m.addVar(vtype=GRB.CONTINUOUS, name=\"A\")\n",
    "LP = m.addVar(vtype=GRB.CONTINUOUS, name=\"LP\")\n",
    "TP = m.addVar(vtype=GRB.CONTINUOUS, name=\"TP\")\n",
    "AP = m.addVar(vtype=GRB.CONTINUOUS, name=\"AP\")\n",
    "TU = m.addVar(vtype=GRB.CONTINUOUS, name=\"TU\")\n",
    "AU = m.addVar(vtype=GRB.CONTINUOUS, name=\"AU\")\n",
    "LR = m.addVar(vtype=GRB.CONTINUOUS, name=\"LR\")\n",
    "TR = m.addVar(vtype=GRB.CONTINUOUS, name=\"TR\")\n",
    "AR = m.addVar(vtype=GRB.CONTINUOUS, name=\"AR\")\n",
    "\n",
    "# Set the objective\n",
    "m.setObjective(86 * R + 93 * U + 106 * P - 78 * Aa - 88 * Ab - 75 * T - 130 * L, GRB.MAXIMIZE)\n",
    "\n",
    "# Add linear constraints\n",
    "m.addConstr(A <= 10000, \"Constraint_A\")\n",
    "m.addConstr(T <= 11000, \"Constraint_T\")\n",
    "m.addConstr(L <= 6000, \"Constraint_L\")\n",
    "m.addConstr(R >= 5000, \"Constraint_R\")\n",
    "m.addConstr(U >= 5000, \"Constraint_U\")\n",
    "m.addConstr(P >= 5000, \"Constraint_P\")\n",
    "\n",
    "m.addConstr(AP + AU + AR == A, \"Constraint_A2_Sum\")\n",
    "m.addConstr(A == Aa + Ab, \"Constraint_A1_Sum\")\n",
    "m.addConstr(LP + TP + AP == P, \"Constraint_P_Sum\")\n",
    "m.addConstr(TU + AU == U, \"Constraint_U_Sum\")\n",
    "m.addConstr(LP + LR == L, \"Constraint_L_Sum\")\n",
    "m.addConstr(TP + TU + TR == T, \"Constraint_T_Sum\")\n",
    "m.addConstr(TR + AR + LR == R, \"Constraint_R_Sum\")\n",
    "\n",
    "# Add quadratic constraints\n",
    "m.addQConstr((AR * OA + 83 * TR + 800 * LR) >= 90*R, \"Quadratic_Constraint_1\")\n",
    "m.addQConstr((AP * OA + 83 * TP + 800 * LP) >= 94*P, \"Quadratic_Constraint_2\")\n",
    "m.addQConstr((AU * OA + TU * 83) >= 88*U, \"Quadratic_Constraint_3\")\n",
    "\n",
    "m.addQConstr(0.04*Aa+0.01*Ab == SA*A, \"Quadratic_Constraint_4\")\n",
    "m.addQConstr(91*Aa+97*Ab == OA*A, \"Quadratic_Constraint_5\")\n",
    "m.addQConstr(SA*AR + 0.02*TR <= 0.03*R, \"Quadratic_Constraint_6\")\n",
    "m.addQConstr(SA*AP + 0.02*TP <= 0.03*P, \"Quadratic_Constraint_7\")\n",
    "m.addQConstr(SA*AU + 0.02*TU <= 0.03*U, \"Quadratic_Constraint_8\")\n",
    "\n",
    "m.setParam('NonConvex', 2)\n",
    "\n",
    "# Optimize the model\n",
    "m.optimize()\n",
    "\n",
    "# Print the solution\n",
    "for v in m.getVars():\n",
    "    print(f'{v.varName}: {v.x}')\n",
    "print(f'Obj: {m.objVal}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "$f(x)=x_1^2 + x_2^2 + 3x_3^2 − x_1x_2 − x_2x_3 − x_1x_3$\n",
    "\n",
    "$\\nabla f(x) = [2x_1 - x_2 - x_3, 2x_2 - x_1 - x_3, 6x_3-x_2-x_1]$\n",
    "\n",
    "$H=\\begin{bmatrix} \\frac{\\partial f^2}{\\partial x_1x_1} & ... & ... \\\\ ... & \\frac{\\partial f^2}{\\partial x_2x_2} & ... \\\\ ... & ... & \\frac{\\partial f^2}{\\partial x_3x_3} \\end{bmatrix}=\\begin{bmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 6 \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62771868 3.         6.37228132]\n"
     ]
    }
   ],
   "source": [
    "m = np.matrix([[2, -1, -1],[-1,2,-1],[-1,-1,6]])\n",
    "print(np.linalg.eigvals(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8. -30.]\n",
      "Set parameter NonConvex to value 2\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (win64)\n",
      "\n",
      "CPU model: 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 0 rows, 2 columns and 0 nonzeros\n",
      "Model fingerprint: 0x301eedd2\n",
      "Model has 2 quadratic objective terms\n",
      "Coefficient statistics:\n",
      "  Matrix range     [0e+00, 0e+00]\n",
      "  Objective range  [6e+01, 1e+02]\n",
      "  QObjective range [8e+00, 3e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [0e+00, 0e+00]\n",
      "Presolve removed 0 rows and 2 columns\n",
      "Presolve time: 0.01s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Barrier solved model in 0 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective 3.92812500e+02\n",
      "Optimal solution found: q1 = 6.875, q2 = 4.5\n",
      "Maximum value of the objective function: 392.8125\n"
     ]
    }
   ],
   "source": [
    "# Using Gurobi to solve given function is concave\n",
    "matrix = np.matrix([[-8,0],[0,-30]])\n",
    "print(np.linalg.eigvals(matrix))\n",
    "\n",
    "# Create a new model\n",
    "m = gp.Model(\"concave_optimization\")\n",
    "\n",
    "# Create variables\n",
    "q1 = m.addVar(vtype=gp.GRB.CONTINUOUS, name=\"q1\")\n",
    "q2 = m.addVar(vtype=gp.GRB.CONTINUOUS, name=\"q2\")\n",
    "\n",
    "# Set the objective function\n",
    "obj = q1 * (70 - 4 * q1) + q2 * (150 - 15 * q2) - 100 - 15 * q1 - 15 * q2\n",
    "m.setObjective(obj, gp.GRB.MAXIMIZE)\n",
    "\n",
    "# Since the objective is a concave function, we need to set the NonConvex parameter\n",
    "m.setParam('NonConvex', 2)\n",
    "\n",
    "# Optimize the model\n",
    "m.optimize()\n",
    "\n",
    "# Print the solution\n",
    "if m.status == gp.GRB.OPTIMAL:\n",
    "    print(f'Optimal solution found: q1 = {q1.x}, q2 = {q2.x}')\n",
    "    print(f'Maximum value of the objective function: {m.objVal}')\n",
    "else:\n",
    "    print(\"No optimal solution found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.87499999 4.49999999]\n"
     ]
    }
   ],
   "source": [
    "# Using Scipy to solve from example\n",
    "x = np.array([1, 1])\n",
    "\n",
    "# Minimize using BFGS\n",
    "res = minimize(lambda x: -(x[0]*(70 - 4*x[0]) + x[1]*(150-15*x[1]) - 100 - 15*x[0] - 15*x[1]), \n",
    "               x, method='SLSQP')\n",
    "\n",
    "if res.success:\n",
    "    print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return np.array([2*(x[0]-3), 2*(x[1]-2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.99999617, 1.99999808]), array([7.66247770e-06, 3.83123885e-06]), 60)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepest_descent(df, x0=[np.array([1, 1])], opt_fun='min', t_opt=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current point is:  [1 1]\n",
      "g = [4 2]\n",
      "The current point is:  [1.4 1.2]\n",
      "g = [3.2 1.6]\n",
      "The current point is:  [1.72 1.36]\n",
      "g = [2.56 1.28]\n",
      "The current point is:  [1.976 1.488]\n",
      "g = [2.048 1.024]\n",
      "The current point is:  [2.1808 1.5904]\n",
      "g = [1.6384 0.8192]\n",
      "The current point is:  [2.34464 1.67232]\n",
      "g = [1.31072 0.65536]\n",
      "The current point is:  [2.475712 1.737856]\n",
      "g = [1.048576 0.524288]\n",
      "The current point is:  [2.5805696 1.7902848]\n",
      "g = [0.8388608 0.4194304]\n",
      "The current point is:  [2.66445568 1.83222784]\n",
      "g = [0.67108864 0.33554432]\n",
      "The current point is:  [2.73156454 1.86578227]\n",
      "g = [0.53687091 0.26843546]\n",
      "The current point is:  [2.78525164 1.89262582]\n",
      "g = [0.42949673 0.21474836]\n",
      "The current point is:  [2.82820131 1.91410065]\n",
      "g = [0.34359738 0.17179869]\n",
      "The current point is:  [2.86256105 1.93128052]\n",
      "g = [0.27487791 0.13743895]\n",
      "The current point is:  [2.89004884 1.94502442]\n",
      "g = [0.21990233 0.10995116]\n",
      "The current point is:  [2.91203907 1.95601953]\n",
      "g = [0.17592186 0.08796093]\n",
      "The current point is:  [2.92963126 1.96481563]\n",
      "g = [0.14073749 0.07036874]\n",
      "The current point is:  [2.943705  1.9718525]\n",
      "g = [0.11258999 0.056295  ]\n",
      "The current point is:  [2.954964 1.977482]\n",
      "g = [0.09007199 0.045036  ]\n",
      "The current point is:  [2.9639712 1.9819856]\n",
      "g = [0.07205759 0.0360288 ]\n",
      "The current point is:  [2.97117696 1.98558848]\n",
      "g = [0.05764608 0.02882304]\n",
      "The current point is:  [2.97694157 1.98847078]\n",
      "g = [0.04611686 0.02305843]\n",
      "The current point is:  [2.98155326 1.99077663]\n",
      "g = [0.03689349 0.01844674]\n",
      "The current point is:  [2.9852426 1.9926213]\n",
      "g = [0.02951479 0.0147574 ]\n",
      "The current point is:  [2.98819408 1.99409704]\n",
      "g = [0.02361183 0.01180592]\n",
      "The current point is:  [2.99055527 1.99527763]\n",
      "g = [0.01888947 0.00944473]\n",
      "The current point is:  [2.99244421 1.99622211]\n",
      "g = [0.01511157 0.00755579]\n",
      "The current point is:  [2.99395537 1.99697769]\n",
      "g = [0.01208926 0.00604463]\n",
      "The current point is:  [2.9951643  1.99758215]\n",
      "g = [0.00967141 0.0048357 ]\n",
      "The current point is:  [2.99613144 1.99806572]\n",
      "g = [0.00773713 0.00386856]\n",
      "The current point is:  [2.99690515 1.99845257]\n",
      "g = [0.0061897  0.00309485]\n",
      "The current point is:  [2.99752412 1.99876206]\n",
      "g = [0.00495176 0.00247588]\n",
      "The current point is:  [2.9980193  1.99900965]\n",
      "g = [0.00396141 0.0019807 ]\n",
      "The current point is:  [2.99841544 1.99920772]\n",
      "g = [0.00316913 0.00158456]\n",
      "The current point is:  [2.99873235 1.99936617]\n",
      "g = [0.0025353  0.00126765]\n",
      "The current point is:  [2.99898588 1.99949294]\n",
      "g = [0.00202824 0.00101412]\n",
      "The current point is:  [2.9991887  1.99959435]\n",
      "g = [0.00162259 0.0008113 ]\n",
      "The current point is:  [2.99935096 1.99967548]\n",
      "g = [0.00129807 0.00064904]\n",
      "The current point is:  [2.99948077 1.99974039]\n",
      "g = [0.00103846 0.00051923]\n",
      "The current point is:  [2.99958462 1.99979231]\n",
      "g = [0.00083077 0.00041538]\n",
      "The current point is:  [2.99966769 1.99983385]\n",
      "g = [0.00066461 0.00033231]\n",
      "The current point is:  [2.99973415 1.99986708]\n",
      "g = [0.00053169 0.00026585]\n",
      "The current point is:  [2.99978732 1.99989366]\n",
      "g = [0.00042535 0.00021268]\n",
      "The current point is:  [2.99982986 1.99991493]\n",
      "g = [0.00034028 0.00017014]\n",
      "The current point is:  [2.99986389 1.99993194]\n",
      "g = [0.00027223 0.00013611]\n",
      "The current point is:  [2.99989111 1.99994555]\n",
      "g = [0.00021778 0.00010889]\n",
      "The current point is:  [2.99991289 1.99995644]\n",
      "g = [1.74224572e-04 8.71122859e-05]\n",
      "The current point is:  [2.99993031 1.99996516]\n",
      "g = [1.39379657e-04 6.96898287e-05]\n",
      "The current point is:  [2.99994425 1.99997212]\n",
      "g = [1.11503726e-04 5.57518630e-05]\n",
      "The current point is:  [2.9999554 1.9999777]\n",
      "g = [8.92029808e-05 4.46014904e-05]\n",
      "The current point is:  [2.99996432 1.99998216]\n",
      "g = [7.13623846e-05 3.56811923e-05]\n",
      "The current point is:  [2.99997146 1.99998573]\n",
      "g = [5.70899077e-05 2.85449539e-05]\n",
      "The current point is:  [2.99997716 1.99998858]\n",
      "g = [4.56719262e-05 2.28359631e-05]\n",
      "The current point is:  [2.99998173 1.99999087]\n",
      "g = [3.65375409e-05 1.82687705e-05]\n",
      "The current point is:  [2.99998538 1.99999269]\n",
      "g = [2.92300327e-05 1.46150164e-05]\n",
      "The current point is:  [2.99998831 1.99999415]\n",
      "g = [2.33840262e-05 1.16920131e-05]\n",
      "The current point is:  [2.99999065 1.99999532]\n",
      "g = [1.87072210e-05 9.35361048e-06]\n",
      "The current point is:  [2.99999252 1.99999626]\n",
      "g = [1.49657768e-05 7.48288838e-06]\n",
      "The current point is:  [2.99999401 1.99999701]\n",
      "g = [1.19726214e-05 5.98631071e-06]\n",
      "The current point is:  [2.99999521 1.99999761]\n",
      "g = [9.57809713e-06 4.78904857e-06]\n",
      "The current point is:  [2.99999617 1.99999808]\n",
      "g = [7.66247770e-06 3.83123885e-06]\n",
      "Numbers of iterations:  59\n"
     ]
    }
   ],
   "source": [
    "# Defines a function f(x), which takes an input array x and returns a scalar value. \n",
    "def f(x):\n",
    "    # Define the objective function\n",
    "    return (x[0]-3) ** 2 + (x[1]-2)**2   # python starts indexing from 0!\n",
    "\n",
    "#Defines the gradient df(x) of f(x). It returns the derivative of f with respect to x.\n",
    "def df(x):\n",
    "    # Define the gradient of the objective function\n",
    "    return np.array([2 * (x[0]-3), 2*(x[1]-2)])\n",
    "\n",
    "#Gradient Descent Initialization\n",
    "points = [np.array([1, 1])]  \n",
    "tol = 1e-6  \n",
    "maxiter = 1000  \n",
    "dxmin = 1e-6  \n",
    "dx = float('Inf')  \n",
    "gnorm = float('Inf')  \n",
    "\n",
    "count = 0  \n",
    "t_opt = 0.1  \n",
    "\n",
    "while (gnorm >= tol and (count <= maxiter and dx >= dxmin)):\n",
    "    x = points[-1]  \n",
    "    g = -1 * df(x)  \n",
    "    next_point = x + t_opt * g  \n",
    "    points.append(next_point)  \n",
    "\n",
    "    # Update termination conditions\n",
    "    gnorm = la.norm(-g) \n",
    "    dx = la.norm(next_point - x)  \n",
    "    count = count + 1  \n",
    "    \n",
    "    # Output information for user\n",
    "    print(\"The current point is: \", x)\n",
    "    print(\"g =\", g)\n",
    "    \n",
    "# Final output to user\n",
    "print(\"Numbers of iterations: \", count - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "\n",
    "$f(x_1,x_2)=(x_1-1)^2+(x_2-1)^2+|x_1+x_2-3|$\n",
    "\n",
    "$\\nabla f(x_1, x_2) = \\begin{bmatrix} [2(x_1-1)+1, 2(x_2-1)+1] \\\\ [2(x_1-1)-1, 2(x_2-1)-1] \\\\ [2(x_1-1)+a, 2(x_2-1)+a] \\end{bmatrix}$\n",
    "\n",
    "where $-1 \\leq a \\leq 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    \n",
    "    grads = np.array([2*(x[0]-1), 2*(x[1]-1)])    \n",
    "    \n",
    "    if x[0] + x[1] - 3 > 0:\n",
    "        return grads + np.array([1, 1])\n",
    "    elif x[0] + x[1] - 3 < 0:\n",
    "        return grads + np.array([-1, -1])\n",
    "    else:\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "1: The current point is:[0 0]\n",
      " => gradient: [3 3]\n",
      "----------\n",
      "2: The current point is:[0.3 0.3]\n",
      " => gradient: [2.4 2.4]\n",
      "----------\n",
      "3: The current point is:[0.54 0.54]\n",
      " => gradient: [1.92 1.92]\n",
      "----------\n",
      "4: The current point is:[0.732 0.732]\n",
      " => gradient: [1.536 1.536]\n",
      "----------\n",
      "5: The current point is:[0.8856 0.8856]\n",
      " => gradient: [1.2288 1.2288]\n",
      "----------\n",
      "6: The current point is:[1.00848 1.00848]\n",
      " => gradient: [0.98304 0.98304]\n",
      "----------\n",
      "7: The current point is:[1.106784 1.106784]\n",
      " => gradient: [0.786432 0.786432]\n",
      "----------\n",
      "8: The current point is:[1.1854272 1.1854272]\n",
      " => gradient: [0.6291456 0.6291456]\n",
      "----------\n",
      "9: The current point is:[1.24834176 1.24834176]\n",
      " => gradient: [0.50331648 0.50331648]\n",
      "----------\n",
      "10: The current point is:[1.29867341 1.29867341]\n",
      " => gradient: [0.40265318 0.40265318]\n",
      "----------\n",
      "11: The current point is:[1.33893873 1.33893873]\n",
      " => gradient: [0.32212255 0.32212255]\n",
      "----------\n",
      "12: The current point is:[1.37115098 1.37115098]\n",
      " => gradient: [0.25769804 0.25769804]\n",
      "----------\n",
      "13: The current point is:[1.39692078 1.39692078]\n",
      " => gradient: [0.20615843 0.20615843]\n",
      "----------\n",
      "14: The current point is:[1.41753663 1.41753663]\n",
      " => gradient: [0.16492674 0.16492674]\n",
      "----------\n",
      "15: The current point is:[1.4340293 1.4340293]\n",
      " => gradient: [0.1319414 0.1319414]\n",
      "----------\n",
      "16: The current point is:[1.44722344 1.44722344]\n",
      " => gradient: [0.10555312 0.10555312]\n",
      "----------\n",
      "17: The current point is:[1.45777875 1.45777875]\n",
      " => gradient: [0.08444249 0.08444249]\n",
      "----------\n",
      "18: The current point is:[1.466223 1.466223]\n",
      " => gradient: [0.06755399 0.06755399]\n",
      "----------\n",
      "19: The current point is:[1.4729784 1.4729784]\n",
      " => gradient: [0.0540432 0.0540432]\n",
      "----------\n",
      "20: The current point is:[1.47838272 1.47838272]\n",
      " => gradient: [0.04323456 0.04323456]\n",
      "----------\n",
      "21: The current point is:[1.48270618 1.48270618]\n",
      " => gradient: [0.03458765 0.03458765]\n",
      "----------\n",
      "22: The current point is:[1.48616494 1.48616494]\n",
      " => gradient: [0.02767012 0.02767012]\n",
      "----------\n",
      "23: The current point is:[1.48893195 1.48893195]\n",
      " => gradient: [0.02213609 0.02213609]\n",
      "----------\n",
      "24: The current point is:[1.49114556 1.49114556]\n",
      " => gradient: [0.01770887 0.01770887]\n",
      "----------\n",
      "25: The current point is:[1.49291645 1.49291645]\n",
      " => gradient: [0.0141671 0.0141671]\n",
      "----------\n",
      "26: The current point is:[1.49433316 1.49433316]\n",
      " => gradient: [0.01133368 0.01133368]\n",
      "----------\n",
      "27: The current point is:[1.49546653 1.49546653]\n",
      " => gradient: [0.00906694 0.00906694]\n",
      "----------\n",
      "28: The current point is:[1.49637322 1.49637322]\n",
      " => gradient: [0.00725355 0.00725355]\n",
      "----------\n",
      "29: The current point is:[1.49709858 1.49709858]\n",
      " => gradient: [0.00580284 0.00580284]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.49709858, 1.49709858]), array([0.00580284, 0.00580284]), 29)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepest_descent(df, [np.array([0, 0])], opt_fun='min', t_opt=0.1, tol=1e-3, dxmin=1e-3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "The current point is: [0 0]\n",
      "Gradient = [3 3]\n",
      "Function value = 5\n",
      "----\n",
      "Iteration: 2\n",
      "The current point is: [0.3 0.3]\n",
      "Gradient = [2.4 2.4]\n",
      "Function value = 3.38\n",
      "----\n",
      "Iteration: 3\n",
      "The current point is: [0.54 0.54]\n",
      "Gradient = [1.92 1.92]\n",
      "Function value = 2.3432\n",
      "----\n",
      "Iteration: 4\n",
      "The current point is: [0.732 0.732]\n",
      "Gradient = [1.536 1.536]\n",
      "Function value = 1.679648\n",
      "----\n",
      "Iteration: 5\n",
      "The current point is: [0.8856 0.8856]\n",
      "Gradient = [1.2288 1.2288]\n",
      "Function value = 1.25497472\n",
      "----\n",
      "Iteration: 6\n",
      "The current point is: [1.00848 1.00848]\n",
      "Gradient = [0.98304 0.98304]\n",
      "Function value = 0.9831838207999999\n",
      "----\n",
      "Iteration: 7\n",
      "The current point is: [1.106784 1.106784]\n",
      "Gradient = [0.786432 0.786432]\n",
      "Function value = 0.809237645312\n",
      "----\n",
      "Iteration: 8\n",
      "The current point is: [1.1854272 1.1854272]\n",
      "Gradient = [0.6291456 0.6291456]\n",
      "Function value = 0.6979120929996802\n",
      "----\n",
      "Iteration: 9\n",
      "The current point is: [1.24834176 1.24834176]\n",
      "Gradient = [0.50331648 0.50331648]\n",
      "Function value = 0.6266637395197953\n",
      "----\n",
      "Iteration: 10\n",
      "The current point is: [1.29867341 1.29867341]\n",
      "Gradient = [0.40265318 0.40265318]\n",
      "Function value = 0.5810647932926689\n",
      "----\n",
      "Iteration: 11\n",
      "The current point is: [1.33893873 1.33893873]\n",
      "Gradient = [0.32212255 0.32212255]\n",
      "Function value = 0.5518814677073082\n",
      "----\n",
      "Iteration: 12\n",
      "The current point is: [1.37115098 1.37115098]\n",
      "Gradient = [0.25769804 0.25769804]\n",
      "Function value = 0.5332041393326772\n",
      "----\n",
      "Iteration: 13\n",
      "The current point is: [1.39692078 1.39692078]\n",
      "Gradient = [0.20615843 0.20615843]\n",
      "Function value = 0.5212506491729134\n",
      "----\n",
      "Iteration: 14\n",
      "The current point is: [1.41753663 1.41753663]\n",
      "Gradient = [0.16492674 0.16492674]\n",
      "Function value = 0.5136004154706646\n",
      "----\n",
      "Iteration: 15\n",
      "The current point is: [1.4340293 1.4340293]\n",
      "Gradient = [0.1319414 0.1319414]\n",
      "Function value = 0.5087042659012253\n",
      "----\n",
      "Iteration: 16\n",
      "The current point is: [1.44722344 1.44722344]\n",
      "Gradient = [0.10555312 0.10555312]\n",
      "Function value = 0.5055707301767842\n",
      "----\n",
      "Iteration: 17\n",
      "The current point is: [1.45777875 1.45777875]\n",
      "Gradient = [0.08444249 0.08444249]\n",
      "Function value = 0.5035652673131419\n",
      "----\n",
      "Iteration: 18\n",
      "The current point is: [1.466223 1.466223]\n",
      "Gradient = [0.06755399 0.06755399]\n",
      "Function value = 0.5022817710804108\n",
      "----\n",
      "Iteration: 19\n",
      "The current point is: [1.4729784 1.4729784]\n",
      "Gradient = [0.0540432 0.0540432]\n",
      "Function value = 0.5014603334914629\n",
      "----\n",
      "Iteration: 20\n",
      "The current point is: [1.47838272 1.47838272]\n",
      "Gradient = [0.04323456 0.04323456]\n",
      "Function value = 0.5009346134345363\n",
      "----\n",
      "Iteration: 21\n",
      "The current point is: [1.48270618 1.48270618]\n",
      "Gradient = [0.03458765 0.03458765]\n",
      "Function value = 0.5005981525981031\n",
      "----\n",
      "Iteration: 22\n",
      "The current point is: [1.48616494 1.48616494]\n",
      "Gradient = [0.02767012 0.02767012]\n",
      "Function value = 0.500382817662786\n",
      "----\n",
      "Iteration: 23\n",
      "The current point is: [1.48893195 1.48893195]\n",
      "Gradient = [0.02213609 0.02213609]\n",
      "Function value = 0.5002450033041831\n",
      "----\n",
      "Iteration: 24\n",
      "The current point is: [1.49114556 1.49114556]\n",
      "Gradient = [0.01770887 0.01770887]\n",
      "Function value = 0.5001568021146772\n",
      "----\n",
      "Iteration: 25\n",
      "The current point is: [1.49291645 1.49291645]\n",
      "Gradient = [0.0141671 0.0141671]\n",
      "Function value = 0.5001003533533934\n",
      "----\n",
      "Iteration: 26\n",
      "The current point is: [1.49433316 1.49433316]\n",
      "Gradient = [0.01133368 0.01133368]\n",
      "Function value = 0.5000642261461717\n",
      "----\n",
      "Iteration: 27\n",
      "The current point is: [1.49546653 1.49546653]\n",
      "Gradient = [0.00906694 0.00906694]\n",
      "Function value = 0.50004110473355\n",
      "----\n",
      "Iteration: 28\n",
      "The current point is: [1.49637322 1.49637322]\n",
      "Gradient = [0.00725355 0.00725355]\n",
      "Function value = 0.5000263070294719\n",
      "----\n",
      "Iteration: 29\n",
      "The current point is: [1.49709858 1.49709858]\n",
      "Gradient = [0.00580284 0.00580284]\n",
      "Function value = 0.500016836498862\n",
      "----\n",
      "Final point: [1.49767886 1.49767886]\n",
      "Final function value: 0.5000107753592717\n",
      "Number of iterations: 29\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def f(x):\n",
    "    return (x[0] - 1)**2 + (x[1] - 1)**2 + np.abs(x[0] + x[1] - 3)\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def df(x):\n",
    "    # Compute the gradient for the quadratic parts\n",
    "    grad = np.array([2 * (x[0] - 1), 2 * (x[1] - 1)])\n",
    "    # Handle the absolute value term\n",
    "    if x[0] + x[1] - 3 > 0:\n",
    "        grad += np.array([1, 1])\n",
    "    elif x[0] + x[1] - 3 < 0:\n",
    "        grad += np.array([-1, -1])\n",
    "    # No else needed since the subgradient at 0 can be any value between -1 and 1.\n",
    "    return grad\n",
    "\n",
    "# Gradient Descent Initialization\n",
    "points = [np.array([0, 0])]  # Changed starting point to [0, 0] as an example\n",
    "tol = 1e-3\n",
    "maxiter = 1000\n",
    "dxmin = 1e-3\n",
    "dx = float('Inf')\n",
    "gnorm = float('Inf')\n",
    "\n",
    "count = 0\n",
    "t_opt = 0.1  # Step size\n",
    "\n",
    "while gnorm >= tol and count <= maxiter and dx >= dxmin:\n",
    "    x = points[-1]\n",
    "    g = -df(x)\n",
    "    next_point = x + t_opt * g\n",
    "    points.append(next_point)\n",
    "\n",
    "    # Update termination conditions\n",
    "    gnorm = la.norm(g)\n",
    "    dx = la.norm(next_point - x)\n",
    "    count += 1\n",
    "\n",
    "    # Output information for user\n",
    "    print(\"Iteration:\", count)\n",
    "    print(\"The current point is:\", x)\n",
    "    print(\"Gradient =\", g)\n",
    "    print(\"Function value =\", f(x))\n",
    "    print(\"----\")\n",
    "\n",
    "# Final output to user\n",
    "print(\"Final point:\", points[-1])\n",
    "print(\"Final function value:\", f(points[-1]))\n",
    "print(\"Number of iterations:\", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -(-2*x[0]**2 - x[1]**2 + x[0]*x[1] + 8*x[0] + 3*x[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: -15.017857142857137\n",
       "     jac: array([-0.74999988, -0.25      ])\n",
       " message: 'Optimization terminated successfully'\n",
       "    nfev: 10\n",
       "     nit: 3\n",
       "    njev: 3\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([2.46428571, 2.60714286])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constraint type: ‘eq’ for equality, ‘ineq’ for inequality.\n",
    "# 3000*x[0] + 1000*x[1] <= 10000\n",
    "cons = ({'type': 'ineq', 'fun': lambda x:  10000 - (3000*x[0] + 1000*x[1])})\n",
    "\n",
    "# And variables must be positive, hence the following bounds:\n",
    "bnds = ((0, None), (0, None))\n",
    "\n",
    "minimize(f, np.array([1, 1]), bounds=bnds, constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution is at x = 2.4642857140033017 and y = 2.6071428579900946\n",
      "The maximum value of the objective function is 15.017857142857142\n",
      "_________________________________________________________________\n",
      "The Lagrangian solution is x = 2.46428 and y = 2.6071\n"
     ]
    }
   ],
   "source": [
    "# Objective function (we want to maximize z, so we minimize -z)\n",
    "def objective(x):\n",
    "    return 2*x[0]**2 + x[1]**2 - x[0]*x[1] - 8*x[0] - 3*x[1]\n",
    "\n",
    "# Constraint equations\n",
    "def constraint(x):\n",
    "    return 3*x[0] + x[1] - 10\n",
    "\n",
    "# Initial guess\n",
    "x0 = [0, 0]\n",
    "\n",
    "# Define the constraint as a dictionary\n",
    "con = {'type': 'eq', 'fun': constraint}\n",
    "\n",
    "# Solve the problem using SLSQP method\n",
    "sol = minimize(objective, x0, constraints=con, method='SLSQP')\n",
    "\n",
    "# Print the results\n",
    "print(f\"The solution is at x = {sol.x[0]} and y = {sol.x[1]}\")\n",
    "print(f\"The maximum value of the objective function is {-sol.fun}\")\n",
    "print(f\"_________________________________________________________________\")\n",
    "print(f\"The Lagrangian solution is x = 2.46428 and y = 2.6071\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(x):\n",
    "    return -(x[0]*(30 - x[0]) + x[1]*(35-x[1]) - x[0]**2 - 2*x[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: -214.5833332471571\n",
       "     jac: array([0.00049591, 0.00081635])\n",
       " message: 'Optimization terminated successfully'\n",
       "    nfev: 24\n",
       "     nit: 8\n",
       "    njev: 8\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([7.50012382, 5.83346937])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x[0] + x[1] <= 20 (chemical avaiable)\n",
    "# x[0]**2 + 2x[1]**2 <= 250 (extraction budget)\n",
    "cons = (\n",
    "    {'type': 'ineq', 'fun': lambda x:  20 - (x[0] + x[1])},\n",
    "    {'type': 'ineq', 'fun': lambda x:  250 - (x[0]**2 - 2*x[1]**2)}\n",
    "       )\n",
    "\n",
    "# And variables must be positive, hence the following bounds:\n",
    "bnds = ((0, None), (0, None))\n",
    "\n",
    "minimize(profit, x0=np.array([0, 0]), bounds=bnds, constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal x1: 7.49999961978661\n",
      "Optimal x2: 5.833333180358289\n",
      "Maximum value of the objective function: 214.58333333333303\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective(x):\n",
    "    # Since it's a maximization problem, we need to negate the function\n",
    "    return -(30*x[0] + 35*x[1] - 2*x[0]**2 - 3*x[1]**2)\n",
    "\n",
    "# Define the constraints\n",
    "def constraint1(x):\n",
    "    return 250 - (x[0]**2 + 2*x[1]**2)\n",
    "\n",
    "def constraint2(x):\n",
    "    return 20 - (x[0] + x[1])\n",
    "\n",
    "# Initial guesses\n",
    "n = 2\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "# Setup the bounds for x1 and x2 (both are non-negative)\n",
    "b = (0, None)\n",
    "bnds = (b, b)\n",
    "\n",
    "# Define the constraints in dictionary format\n",
    "con1 = {'type': 'ineq', 'fun': constraint1}\n",
    "con2 = {'type': 'ineq', 'fun': constraint2}\n",
    "cons = [con1, con2]\n",
    "\n",
    "# Run the optimization\n",
    "sol = minimize(objective, x0, method='SLSQP', bounds=bnds, constraints=cons)\n",
    "\n",
    "# Optimal values\n",
    "x1_optimal, x2_optimal = sol.x\n",
    "objective_value = -sol.fun\n",
    "\n",
    "print(\"Optimal x1:\", x1_optimal)\n",
    "print(\"Optimal x2:\", x2_optimal)\n",
    "print(\"Maximum value of the objective function:\", objective_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three stocks $S_i$, for $i\\in[1,2,3]$, and define our $E[S_i]$ as the expected annual return for stock $i$. We also provide the $Var(S_i)$ and $Cov(S_i,Sj), \\forall i \\neq j$. We will define our decision variable $x_i$ as the allocation for each stock, given the constraint of $1000$. We will define our portfolio as $P$.  \n",
    "\n",
    "\n",
    "**Primer:**\n",
    "\n",
    "For some random variable $X$ we define our variance as: \\\n",
    "$Var(X) = E[(X-E[X])^2] = E[X^2] - E[X]^2$\n",
    "\n",
    "$E[X] = \\int_{-\\infty}^{-\\infty}xf(x)dx$\n",
    "\n",
    "$E[X^2] = \\int_{-\\infty}^{-\\infty}x^2f(x)dx$\n",
    "\n",
    "A few rules to keep in mind for Variances (similar to the linearity of expecations).\n",
    "\n",
    "$Var(\\alpha X) = \\alpha^2 Var(X)$\n",
    "\n",
    "Review on covariance matrix\n",
    "\n",
    "$Cov(X,Y) = E[(X-E[X])(Y-E[Y])]$ \n",
    "\n",
    "$Cov(X,Y) = E[XY - E[X]Y - XE[Y] + E[X]E[Y]] = E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] = E[XY] - E[X]E[Y]$ \n",
    "\n",
    "$Cov(X,X)= Var(X) = E[X(X) - E[X]X - XE[X] + E[X]E[X]] = E[X(X)] - E[X]E[X] - E[X]E[X] + E[X]E[X] = E[X^2]-E[X]^2$ \n",
    "\n",
    "Application to problem\n",
    "\n",
    "$Var(X+Y) \\neq Var(X) + Var(Y)$ (only when $X,Y$ are independent)\n",
    "\n",
    "$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$ \n",
    "\n",
    "$Var(X+Y) = E[((X+Y)-E[X+Y])^2] = E[(X+Y)^2] - (E[X+Y])^2$\n",
    "\n",
    "$(E[X+Y])^2 = (E[X]+E[Y])^2 = E[X]^2+E[Y]^2+2E[X]E[Y]$\n",
    "\n",
    "$E[(X+Y)^2] = E[X^2 + Y^2 + 2XY] = E[X^2] + E[Y^2] + 2E[XY]$\n",
    "\n",
    "$Var(X+Y) = E[((X+Y)-E[X+Y])^2] = E[(X+Y)^2] - (E[X+Y])^2 = E[X^2] + E[Y^2] + 2E[XY] - (E[X]^2+E[Y]^2+2E[X]E[Y])$\n",
    "\n",
    "$Var(X+Y) = (E[X^2]-E[X]^2) + (E[Y^2]-E[Y]^2) + 2(E[XY] - E[X]E[Y])$\n",
    "\n",
    "$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$ \n",
    "\n",
    "**Objective**\n",
    "\n",
    "$min \\ \\ \\ Var(P) = Var(x_1S_1+x_2S_2+x_3S_3) = w' \\Sigma w $\n",
    "\n",
    "where \\Sigma is your covariance matrix, where variances are the diagonal\n",
    "\n",
    "$\\Sigma=\\begin{bmatrix} Var(S_1) & Cov(S_1,S_2) & Cov(S_1,S_3) \\\\ Cov(S_2, S_1) & Var(S_2) & Cov(S_2, S_3) \\\\ Cov(S_3,S_1) & Cov(S_3, S_2) & Var(S_3) \\end{bmatrix}=\\begin{bmatrix} 0.2 & 0.05 & 0.02 \\\\ 0.05 & 0.08 & 0.03 \\\\ 0.02 & 0.03 & .18 \\end{bmatrix}$\n",
    "\n",
    "$w' = \\begin{bmatrix} x_1, x_2, x_3 \\end{bmatrix}$\n",
    "\n",
    "**Constraint**\n",
    "\n",
    "$\\sum_{i=1} x_i\\cdot E[S_i] \\geq 0.12$\n",
    "\n",
    "$\\sum_{i=1} x_i = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.07523809526827552\n",
      "     jac: array([0.20571429, 0.12285714, 0.0952381 ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 20\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.38095238, 0.47619047, 0.14285715])\n",
      "Dollar allocations for each stock are: [380.95238276 476.19047007 142.85714717]\n",
      "Min Variance was 0.07523809526827552\n"
     ]
    }
   ],
   "source": [
    "# Using Scipy to solve the expression\n",
    "cov_matrix = np.array([[0.2, 0.05, 0.02], [0.05, 0.08, 0.03], [0.02, 0.03, 0.18]])\n",
    "exp_ret = np.array([0.14, 0.11, 0.10]) \n",
    "start_vector = np.array([1, 1, 1])\n",
    "\n",
    "# optimize function for minmimum variance\n",
    "def min_var(x):\n",
    "    return x.dot(cov_matrix).dot(x)\n",
    "\n",
    "# CONSTRAINTS\n",
    "# m1x[0] + m2x[1] + m3 >= 0.12 (chemical avaiable)\n",
    "# x[0] + x[1] + x[2] = 1 (total portfolio constraint)\n",
    "cons = (\n",
    "    {'type': 'ineq', 'fun': lambda x:  x.dot(exp_ret) - 0.12}, \n",
    "    {'type': 'eq', 'fun': lambda x:  x.sum() - 1}\n",
    "       )\n",
    "\n",
    "# And variables must be positive, hence the following bounds:\n",
    "bnds = ((0, None), (0, None),  (0, None))\n",
    "\n",
    "res = minimize(min_var, x0=start_vector, bounds=bnds, constraints=cons)\n",
    "print(res)\n",
    "print(f'Dollar allocations for each stock are: {res.x * 1000}')\n",
    "print(f'Min Variance was {res.fun}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (win64)\n",
      "\n",
      "CPU model: 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 2 rows, 3 columns and 6 nonzeros\n",
      "Model fingerprint: 0xe756fce7\n",
      "Model has 6 quadratic objective terms\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [8e-02, 4e-01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+02, 1e+03]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 2 rows, 3 columns, 6 nonzeros\n",
      "Presolved model has 6 quadratic objective terms\n",
      "Ordering time: 0.00s\n",
      "\n",
      "Barrier statistics:\n",
      " Free vars  : 2\n",
      " AA' NZ     : 6.000e+00\n",
      " Factor NZ  : 1.000e+01\n",
      " Factor Ops : 3.000e+01 (less than 1 second per iteration)\n",
      " Threads    : 1\n",
      "\n",
      "                  Objective                Residual\n",
      "Iter       Primal          Dual         Primal    Dual     Compl     Time\n",
      "   0   1.07990217e+06 -1.07990217e+06  2.50e+03 4.91e-04  1.31e+06     0s\n",
      "   1   7.08353908e+04 -3.63046155e+05  2.96e+01 5.81e-06  1.20e+05     0s\n",
      "   2   8.91456992e+04 -2.09844015e+04  2.96e-05 5.80e-12  2.75e+04     0s\n",
      "   3   7.83606319e+04  7.24984062e+04  3.86e-07 5.68e-14  1.47e+03     0s\n",
      "   4   7.53041653e+04  7.51969736e+04  1.48e-12 7.11e-14  2.68e+01     0s\n",
      "   5   7.52381625e+04  7.52380539e+04  1.59e-12 2.84e-14  2.72e-02     0s\n",
      "   6   7.52380953e+04  7.52380952e+04  1.82e-12 3.07e-14  2.72e-05     0s\n",
      "   7   7.52380952e+04  7.52380952e+04  4.26e-13 3.57e-14  2.72e-08     0s\n",
      "   8   7.52380952e+04  7.52380952e+04  6.82e-13 8.53e-14  2.72e-11     0s\n",
      "\n",
      "Barrier solved model in 8 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective 7.52380952e+04\n",
      "\n",
      "Optimal Values:\n",
      "x1: 380.95238095238153\n",
      "x2: 476.1904761904751\n",
      "x3: 142.8571428571424\n",
      "Minimized z: 75238.09523809534\n"
     ]
    }
   ],
   "source": [
    "# Create a new model\n",
    "m = Model(\"qp\")\n",
    "\n",
    "# Create variables\n",
    "x1 = m.addVar(lb=0, name=\"x1\")\n",
    "x2 = m.addVar(lb=0, name=\"x2\")\n",
    "x3 = m.addVar(lb=0, name=\"x3\")\n",
    "\n",
    "# Set objective\n",
    "m.setObjective(0.20*x1*x1 + 0.08*x2*x2 + 0.18*x3*x3 + 0.10*x1*x2 + 0.04*x1*x3 + 0.06*x2*x3, GRB.MINIMIZE)\n",
    "\n",
    "# Add constraints\n",
    "m.addConstr(0.14*x1 + 0.11*x2 + 0.10*x3 >= 120, \"c0\")\n",
    "m.addConstr(x1 + x2 + x3 == 1000, \"c1\")\n",
    "\n",
    "# Optimize model\n",
    "m.optimize()\n",
    "\n",
    "# Print solution\n",
    "print('Optimal Values:')\n",
    "print('x1:', x1.X)\n",
    "print('x2:', x2.X)\n",
    "print('x3:', x3.X)\n",
    "print('Minimized z:', m.ObjVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
